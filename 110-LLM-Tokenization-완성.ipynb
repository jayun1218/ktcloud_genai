{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_intVJo1FXQF"
   },
   "source": [
    "# **Tokenization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81_Ybs4LI7IX"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- 💡 **NOTE**\n",
    "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8ya0DICf_mC"
   },
   "source": [
    "# **Tokenization 이란?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2_OuRR6LLCG"
   },
   "source": [
    "- 텍스트를 컴퓨터가 처리할 수 있는 의미있는 단위(토큰)로 나누는 전처리 과정\n",
    "- 기본 아이디어\n",
    "    - 텍스트 → 토큰 리스트 변환\n",
    "    - AI 모델이 이해할 수 있는 최소 의미 단위로 분할\n",
    "    - 언어마다 다른 방식 적용\n",
    "- 토큰화 방법\n",
    "\n",
    "|방법|설명|예시|장점|단점|\n",
    "|---|---|---|---|---|\n",
    "|단어 단위|공백으로 분할\"|I love AI\" → [\"I\", \"love\", \"AI\"]|직관적어휘 크기 폭발|\n",
    "|문자 단위|글자별 분할|\"Hello\" → [\"H\", \"e\", \"l\", \"l\", \"o\"]|어휘 크기 작음|의미 정보 손실|\n",
    "|서브워드|단어를 더 작은 단위로|\"unknown\" → [\"un\", \"know\", \"n\"]|균형잡힌 접근|복잡한 알고리즘|\n",
    "|형태소 분석|의미 단위로 분할|\"먹었다\" → [\"먹\", \"었\", \"다\"]|언어학적 정확성|언어별 특화 필요|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m2T0NYhIG8H"
   },
   "source": [
    "### 예제 1: 기본 토큰화 방법들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758724150064,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "eH2TnXbxIHGx",
    "outputId": "66b98d07-8ddf-4d10-8ced-27a9f07f1a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 기본 토큰화 방법들 ===\n",
      "원본 텍스트: Hello, World! I'm learning NLP. It's amazing!\n",
      "\n",
      "1. 공백 분할: ['Hello,', 'World!', \"I'm\", 'learning', 'NLP.', \"It's\", 'amazing!']\n",
      "2. 단어만 추출: ['hello', 'world', 'i', 'm', 'learning', 'nlp', 'it', 's', 'amazing']\n",
      "3. 문자 분할: ['h', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'i', \"'\", 'm', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'n', 'l', 'p', '.', ' ', 'i', 't', \"'\", 's', ' ', 'a', 'm', 'a', 'z', 'i', 'n', 'g', '!']\n",
      "4. 구두점 포함: ['Hello', ',', 'World', '!', 'I', \"'\", 'm', 'learning', 'NLP', '.', 'It', \"'\", 's', 'amazing', '!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def basic_tokenization_examples():\n",
    "    text = \"Hello, World! I'm learning NLP. It's amazing!\"\n",
    "\n",
    "    print(\"=== 기본 토큰화 방법들 ===\")\n",
    "    print(f\"원본 텍스트: {text}\")\n",
    "\n",
    "    # 1. 공백 기반 분할\n",
    "    whitespace_tokens = text.split()\n",
    "    print(f\"\\n1. 공백 분할: {whitespace_tokens}\")\n",
    "\n",
    "    # 2. 정규식으로 단어 추출\n",
    "    word_tokens = re.findall(r'\\w+', text.lower())\n",
    "    print(f\"2. 단어만 추출: {word_tokens}\")\n",
    "\n",
    "    # 3. 문자 단위 분할\n",
    "    char_tokens = list(text.lower())\n",
    "    print(f\"3. 문자 분할: {char_tokens}\")\n",
    "\n",
    "    # 4. 구두점 포함 분할\n",
    "    punct_tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    print(f\"4. 구두점 포함: {punct_tokens}\")\n",
    "\n",
    "basic_tokenization_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEq8FkV8IOEy"
   },
   "source": [
    "### 예제 2: 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1758724163216,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "GF_FDBu-IP9I",
    "outputId": "4b9c7255-b90d-4738-db04-80996ce20d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 한국어 토큰화 ===\n",
      "원본 텍스트: 안녕하세요! 저는 한국어를 공부하고 있습니다.\n",
      "1. 공백 분할: ['안녕하세요!', '저는', '한국어를', '공부하고', '있습니다.']\n",
      "2. 문자 분할: ['안', '녕', '하', '세', '요', '!', ' ', '저', '는', ' ', '한', '국', '어', '를', ' ', '공', '부', '하', '고', ' ', '있', '습', '니', '다', '.']\n",
      "3. 간단한 분할: ['안녕하세요', '!', '저는', '한국어를', '공부하고', '있습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "def korean_tokenization():\n",
    "    korean_text = \"안녕하세요! 저는 한국어를 공부하고 있습니다.\"\n",
    "\n",
    "    print(\"\\n=== 한국어 토큰화 ===\")\n",
    "    print(f\"원본 텍스트: {korean_text}\")\n",
    "\n",
    "    # 1. 공백 기반 (한국어는 부정확)\n",
    "    space_tokens = korean_text.split()\n",
    "    print(f\"1. 공백 분할: {space_tokens}\")\n",
    "\n",
    "    # 2. 문자 단위\n",
    "    char_tokens = list(korean_text)\n",
    "    print(f\"2. 문자 분할: {char_tokens}\")\n",
    "\n",
    "    # 3. 간단한 한국어 토큰화 (실제로는 형태소 분석기 사용)\n",
    "    # 여기서는 어절 단위로 분리\n",
    "    simple_tokens = re.findall(r'[가-힣]+|[a-zA-Z]+|[!?.]', korean_text)\n",
    "    print(f\"3. 간단한 분할: {simple_tokens}\")\n",
    "\n",
    "korean_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aWpo7ejISlq"
   },
   "source": [
    "### 예제 3: 서브워드 토큰화 시뮬레이션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1758724183751,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "7MF_j6FjITvt",
    "outputId": "79a905f5-a38b-4752-b484-d313a736903c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 서브워드 토큰화 데모 ===\n",
      "추가된 서브워드: 'un' (빈도: 2)\n",
      "추가된 서브워드: 're' (빈도: 2)\n",
      "추가된 서브워드: 'ep' (빈도: 2)\n",
      "추가된 서브워드: 'pr' (빈도: 2)\n",
      "추가된 서브워드: 'nk' (빈도: 1)\n",
      "\n",
      "최종 어휘: ['a', 'c', 'e', 'ep', 'g', 'h', 'i', 'k', 'l', 'n', 'nk', 'o', 'p', 'pr', 'r', 're', 's', 'u', 'un', 'w', 'y']\n",
      "\n",
      "=== 서브워드 토큰화 결과 ===\n",
      "unknown         → ['un', 'k', 'n', 'o', 'w', 'n']\n",
      "unhappy         → ['un', 'h', 'a', 'p', 'p', 'y']\n",
      "replay          → ['re', 'p', 'l', 'a', 'y']\n",
      "preprocessing   → ['pr', 'ep', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "def subword_tokenization_demo():\n",
    "    words = [\"unknown\", \"unhappy\", \"replay\", \"preprocessing\"]\n",
    "\n",
    "    print(\"\\n=== 서브워드 토큰화 데모 ===\")\n",
    "\n",
    "    # 간단한 BPE 시뮬레이션\n",
    "    vocab = set()\n",
    "\n",
    "    # 모든 문자를 기본 어휘에 추가\n",
    "    for word in words:\n",
    "        vocab.update(list(word))\n",
    "\n",
    "    # 자주 나오는 문자 조합 찾기\n",
    "    bigrams = Counter()\n",
    "    for word in words:\n",
    "        for i in range(len(word) - 1):\n",
    "            bigrams[word[i:i+2]] += 1\n",
    "\n",
    "    # 가장 빈번한 바이그램을 어휘에 추가\n",
    "    common_bigrams = bigrams.most_common(5)\n",
    "    for bigram, count in common_bigrams:\n",
    "        vocab.add(bigram)\n",
    "        print(f\"추가된 서브워드: '{bigram}' (빈도: {count})\")\n",
    "\n",
    "    print(f\"\\n최종 어휘: {sorted(vocab)}\")\n",
    "\n",
    "    # 단어를 서브워드로 분해 (단순화된 버전)\n",
    "    def tokenize_word(word):\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            # 가장 긴 매칭 찾기\n",
    "            found = False\n",
    "            for length in range(min(3, len(word) - i), 0, -1):\n",
    "                subword = word[i:i+length]\n",
    "                if subword in vocab:\n",
    "                    tokens.append(subword)\n",
    "                    i += length\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                tokens.append(word[i])\n",
    "                i += 1\n",
    "        return tokens\n",
    "\n",
    "    print(f\"\\n=== 서브워드 토큰화 결과 ===\")\n",
    "    for word in words:\n",
    "        tokens = tokenize_word(word)\n",
    "        print(f\"{word:15} → {tokens}\")\n",
    "\n",
    "subword_tokenization_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuAIVwlRIcEG"
   },
   "source": [
    "### 예제 4: 실제 AI 모델에서 사용하는 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1758724219983,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "-hafV4SlIdFQ",
    "outputId": "a2eb565a-f37f-474c-fa9d-0efbea955a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AI 모델용 토큰화 ===\n",
      "어휘 사전 크기: 16\n",
      "어휘: ['<pad>', '<unk>', '<start>', '<end>', 'i', 'love', 'artificial', 'intelligence', 'machine', 'learning', 'is', 'fascinating', 'natural', 'language', 'processing', 'rocks']\n",
      "\n",
      "=== 토큰 ID 변환 ===\n",
      "'I love artificial intelligence!'\n",
      "토큰 ID: [2, 4, 5, 6, 7, 3, 0, 0, 0, 0]\n",
      "토큰: ['<start>', 'i', 'love', 'artificial', 'intelligence', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "'Machine learning is fascinating.'\n",
      "토큰 ID: [2, 8, 9, 10, 11, 3, 0, 0, 0, 0]\n",
      "토큰: ['<start>', 'machine', 'learning', 'is', 'fascinating', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n",
      "'Natural language processing rocks!'\n",
      "토큰 ID: [2, 12, 13, 14, 15, 3, 0, 0, 0, 0]\n",
      "토큰: ['<start>', 'natural', 'language', 'processing', 'rocks', '<end>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ai_model_tokenization():\n",
    "    sentences = [\n",
    "        \"I love artificial intelligence!\",\n",
    "        \"Machine learning is fascinating.\",\n",
    "        \"Natural language processing rocks!\"\n",
    "    ]\n",
    "\n",
    "    print(\"\\n=== AI 모델용 토큰화 ===\")\n",
    "\n",
    "    # 1. 어휘 사전 구축\n",
    "    word_counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        words = re.findall(r'\\w+', sentence.lower())\n",
    "        word_counter.update(words)\n",
    "\n",
    "    # 2. 특수 토큰 추가\n",
    "    vocab = {\n",
    "        '<pad>': 0,    # 패딩\n",
    "        '<unk>': 1,    # 미지 단어\n",
    "        '<start>': 2,  # 시작\n",
    "        '<end>': 3     # 끝\n",
    "    }\n",
    "\n",
    "    # 3. 빈번한 단어들 추가\n",
    "    for word, count in word_counter.most_common():\n",
    "        if count >= 1:  # 최소 빈도\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    print(f\"어휘 사전 크기: {len(vocab)}\")\n",
    "    print(f\"어휘: {list(vocab.keys())}\")\n",
    "\n",
    "    # 4. 문장을 토큰 ID로 변환\n",
    "    def sentence_to_ids(sentence, vocab, max_len=10):\n",
    "        words = re.findall(r'\\w+', sentence.lower())\n",
    "        ids = [vocab['<start>']]\n",
    "\n",
    "        for word in words:\n",
    "            if len(ids) >= max_len - 1:  # <end> 공간 확보\n",
    "                break\n",
    "            ids.append(vocab.get(word, vocab['<unk>']))\n",
    "\n",
    "        ids.append(vocab['<end>'])\n",
    "\n",
    "        # 패딩\n",
    "        while len(ids) < max_len:\n",
    "            ids.append(vocab['<pad>'])\n",
    "\n",
    "        return ids[:max_len]\n",
    "\n",
    "    print(f\"\\n=== 토큰 ID 변환 ===\")\n",
    "    for sentence in sentences:\n",
    "        token_ids = sentence_to_ids(sentence, vocab)\n",
    "        print(f\"'{sentence}'\")\n",
    "        print(f\"토큰 ID: {token_ids}\")\n",
    "\n",
    "        # 역변환으로 확인\n",
    "        id_to_vocab = {v: k for k, v in vocab.items()}\n",
    "        recovered_tokens = [id_to_vocab[id] for id in token_ids]\n",
    "        print(f\"토큰: {recovered_tokens}\")\n",
    "        print()\n",
    "\n",
    "ai_model_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOWBFhLLI3ZD"
   },
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMbMTp09I71r"
   },
   "source": [
    "# 현대 AI에서 사용하는 토크나이저\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sTcbqOgJCFo"
   },
   "source": [
    "|토크나이저|사용 모델|특징\n",
    "|---|---|---|\n",
    "|BPE|GPT 시리즈|바이트 쌍 인코딩|\n",
    "|SentencePiece|T5, mT5|언어 무관한 서브워드|\n",
    "|WordPiece|BERT|구글 개발, 한국어 지원|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWLg6zh1JTqC"
   },
   "source": [
    "- 핵심 포인트\n",
    "    - 전처리의 첫 단계: 모든 NLP 태스크의 시작점\n",
    "    - 언어별 최적화: 한국어는 형태소 분석 필수\n",
    "    - 균형점 찾기: 의미 vs 어휘 크기 vs 계산 효율성\n",
    "    - 특수 토큰: <pad>, <unk>, <start>, <end> 등으로 모델 제어\n",
    "\n",
    "- 비유:\n",
    "    - \"문장을 레고 블록으로 분해하는 것\" - 컴퓨터가 조립할 수 있는 크기로!\n",
    "    - 토큰화는 모든 NLP 모델의 입구이며, 성능에 큰 영향을 미치는 중요한 전처리 과정이다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOlNvoVKJj7L"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSjsIcSfJkoh"
   },
   "source": [
    "# Transformers 토크나이저 사용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbZjTUHSgJiJ"
   },
   "source": [
    "### **예제 1: 토크나이제이션 과정 단계별 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK9BCOKOg6SK"
   },
   "outputs": [],
   "source": [
    "# 경고 메시지만 숨기기\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# 1. 경고 메시지 비활성화\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 2. Hugging Face 진행률 표시 끄기\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_ssqmvCw_-k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26636,
     "status": "ok",
     "timestamp": 1758013516444,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "Gk6peed0gYNy",
    "outputId": "bf7bd632-9dab-447d-a694-6a5a169cfa23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 원본 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
      "\n",
      "1️⃣ 토큰 분할 결과: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ']\n",
      "\n",
      "2️⃣ 토큰 ID들: [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97, 13, 220, 169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222]\n",
      "\n",
      "3️⃣ encode 결과 (리스트): [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97, 13, 220, 169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222]\n",
      "\n",
      "4️⃣ 텐서 형태로 변환\n",
      "⭢ input_ids (텐서): tensor([[  169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
      "           252,   246,   167,   108,   235, 35975,   222, 23821,   252,   105,\n",
      "           167,   107,   116,   168,   252,   230, 46695,    97,    13,   220,\n",
      "           169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
      "           252,   246,   167,   108,   235, 35975,   222]])\n",
      "⭢ input_ids 형태: torch.Size([1, 47])\n",
      "⭢ input_ids 타입: <class 'torch.Tensor'>\n",
      "\n",
      "✅ 역변환 결과: '프로그래밍은 재미있다. 프로그래밍은'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2') # 무료 공개 모델\n",
    "\n",
    "# 원본 텍스트\n",
    "input_text = \"프로그래밍은 재미있다. 프로그래밍은\"\n",
    "print(f\"✅ 원본 텍스트: '{input_text}'\")\n",
    "print()\n",
    "\n",
    "# 1단계: 토큰으로 분할 (문자열 형태)\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "print(f\"1️⃣ 토큰 분할 결과: {tokens}\")\n",
    "print()\n",
    "\n",
    "# 2단계: 각 토큰의 ID 확인\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"2️⃣ 토큰 ID들: {token_ids}\")\n",
    "print()\n",
    "\n",
    "# 3단계: encode 함수로 한번에 처리\n",
    "input_ids_list = tokenizer.encode(input_text)\n",
    "print(f\"3️⃣ encode 결과 (리스트): {input_ids_list}\")\n",
    "print()\n",
    "\n",
    "# 4단계: 텐서 형태로 변환\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(f\"4️⃣ 텐서 형태로 변환\")\n",
    "print(f\"⭢ input_ids (텐서): {input_ids}\")\n",
    "print(f\"⭢ input_ids 형태: {input_ids.shape}\")\n",
    "print(f\"⭢ input_ids 타입: {type(input_ids)}\")\n",
    "print()\n",
    "\n",
    "# 역변환: ID를 다시 텍스트로\n",
    "decoded_text = tokenizer.decode(input_ids[0])\n",
    "print(f\"✅ 역변환 결과: '{decoded_text}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPnBbz4XgSHJ"
   },
   "source": [
    "## **예제 2: 다양한 텍스트의 토크나이제이션 비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1758013593823,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "1MBOshvUkB_s",
    "outputId": "2b2d6738-3669-483c-b42c-e32f2f0e9696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 다양한 텍스트의 토크나이제이션 결과 ===\n",
      "텍스트: '안녕하세요'\n",
      "토큰: ['ì', 'ķ', 'Ī', 'ë', 'ħ', 'ķ', 'íķ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ']\n",
      "input_ids: [[168, 243, 230, 167, 227, 243, 47991, 246, 168, 226, 116, 168, 248, 242]]\n",
      "토큰 개수: 14\n",
      "--------------------------------------------------\n",
      "텍스트: 'Hello world'\n",
      "토큰: ['Hello', 'Ġworld']\n",
      "input_ids: [[15496, 995]]\n",
      "토큰 개수: 2\n",
      "--------------------------------------------------\n",
      "텍스트: '프로그래밍'\n",
      "토큰: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į']\n",
      "input_ids: [[169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235]]\n",
      "토큰 개수: 15\n",
      "--------------------------------------------------\n",
      "텍스트: 'AI는 미래다'\n",
      "토큰: ['AI', 'ë', 'Ĭ', 'Ķ', 'Ġë', '¯', '¸', 'ë', 'ŀ', 'ĺ', 'ëĭ', '¤']\n",
      "input_ids: [[20185, 167, 232, 242, 31619, 107, 116, 167, 252, 246, 46695, 97]]\n",
      "토큰 개수: 12\n",
      "--------------------------------------------------\n",
      "텍스트: '123456'\n",
      "토큰: ['123', '456']\n",
      "input_ids: [[10163, 29228]]\n",
      "토큰 개수: 2\n",
      "--------------------------------------------------\n",
      "텍스트: 'hello@email.com'\n",
      "토큰: ['hello', '@', 'email', '.', 'com']\n",
      "input_ids: [[31373, 31, 12888, 13, 785]]\n",
      "토큰 개수: 5\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 다양한 텍스트 예제\n",
    "texts = [\n",
    "    \"안녕하세요\",\n",
    "    \"Hello world\",\n",
    "    \"프로그래밍\",\n",
    "    \"AI는 미래다\",\n",
    "    \"123456\",\n",
    "    \"hello@email.com\"\n",
    "]\n",
    "\n",
    "print(\"=== 다양한 텍스트의 토크나이제이션 결과 ===\")\n",
    "for text in texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    print(f\"텍스트: '{text}'\")\n",
    "    print(f\"토큰: {tokens}\")\n",
    "    print(f\"input_ids: {input_ids.tolist()}\")\n",
    "    print(f\"토큰 개수: {len(tokens)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU3ZzrJImaQn"
   },
   "source": [
    "## **예제 3: 토큰 ID의 사용 과정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16608,
     "status": "ok",
     "timestamp": 1758013655710,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "S-nY1StkmiJZ",
    "outputId": "f0f468f1-e6f2-4768-d84c-b736a0ad9d6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
      "input_ids: tensor([[  169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
      "           252,   246,   167,   108,   235, 35975,   222, 23821,   252,   105,\n",
      "           167,   107,   116,   168,   252,   230, 46695,    97,    13,   220,\n",
      "           169,   242,   226,   167,    94,   250,   166,   115,   116,   167,\n",
      "           252,   246,   167,   108,   235, 35975,   222]])\n",
      "각 ID가 나타내는 토큰:\n",
      "  위치 0: ID 169 → '�'\n",
      "  위치 1: ID 242 → '�'\n",
      "  위치 2: ID 226 → '�'\n",
      "  위치 3: ID 167 → '�'\n",
      "  위치 4: ID 94 → '�'\n",
      "  위치 5: ID 250 → '�'\n",
      "  위치 6: ID 166 → '�'\n",
      "  위치 7: ID 115 → '�'\n",
      "  위치 8: ID 116 → '�'\n",
      "  위치 9: ID 167 → '�'\n",
      "  위치 10: ID 252 → '�'\n",
      "  위치 11: ID 246 → '�'\n",
      "  위치 12: ID 167 → '�'\n",
      "  위치 13: ID 108 → '�'\n",
      "  위치 14: ID 235 → '�'\n",
      "  위치 15: ID 35975 → '�'\n",
      "  위치 16: ID 222 → '�'\n",
      "  위치 17: ID 23821 → ' �'\n",
      "  위치 18: ID 252 → '�'\n",
      "  위치 19: ID 105 → '�'\n",
      "  위치 20: ID 167 → '�'\n",
      "  위치 21: ID 107 → '�'\n",
      "  위치 22: ID 116 → '�'\n",
      "  위치 23: ID 168 → '�'\n",
      "  위치 24: ID 252 → '�'\n",
      "  위치 25: ID 230 → '�'\n",
      "  위치 26: ID 46695 → '�'\n",
      "  위치 27: ID 97 → '�'\n",
      "  위치 28: ID 13 → '.'\n",
      "  위치 29: ID 220 → ' '\n",
      "  위치 30: ID 169 → '�'\n",
      "  위치 31: ID 242 → '�'\n",
      "  위치 32: ID 226 → '�'\n",
      "  위치 33: ID 167 → '�'\n",
      "  위치 34: ID 94 → '�'\n",
      "  위치 35: ID 250 → '�'\n",
      "  위치 36: ID 166 → '�'\n",
      "  위치 37: ID 115 → '�'\n",
      "  위치 38: ID 116 → '�'\n",
      "  위치 39: ID 167 → '�'\n",
      "  위치 40: ID 252 → '�'\n",
      "  위치 41: ID 246 → '�'\n",
      "  위치 42: ID 167 → '�'\n",
      "  위치 43: ID 108 → '�'\n",
      "  위치 44: ID 235 → '�'\n",
      "  위치 45: ID 35975 → '�'\n",
      "  위치 46: ID 222 → '�'\n",
      "\n",
      "모델 출력 형태: torch.Size([1, 47, 50257])\n",
      "마지막 토큰 위치의 확률 분포 크기: torch.Size([50257])\n",
      "\n",
      "다음 토큰 예측 상위 5개:\n",
      "1. ' �' (확률: 0.8128)\n",
      "2. ' �' (확률: 0.1152)\n",
      "3. ' ' (확률: 0.0436)\n",
      "4. '�' (확률: 0.0058)\n",
      "5. '�' (확률: 0.0029)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# 모델과 토크나이저 로드\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 원본 텍스트\n",
    "input_text = \"프로그래밍은 재미있다. 프로그래밍은\"\n",
    "print(f\"입력 텍스트: '{input_text}'\")\n",
    "\n",
    "# 토크나이제이션\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(f\"input_ids: {input_ids}\")\n",
    "print(f\"각 ID가 나타내는 토큰:\")\n",
    "\n",
    "# 각 ID가 무슨 토큰인지 확인\n",
    "for i, token_id in enumerate(input_ids[0]):\n",
    "    token = tokenizer.decode([token_id])\n",
    "    print(f\"  위치 {i}: ID {token_id.item()} → '{token}'\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 모델에 입력하여 다음 토큰 확률 계산\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    # logits: [배치_크기, 시퀀스_길이, 어휘_크기]\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(f\"모델 출력 형태: {logits.shape}\")\n",
    "print(f\"마지막 토큰 위치의 확률 분포 크기: {logits[0, -1, :].shape}\")\n",
    "\n",
    "# 다음 토큰으로 가능성이 높은 상위 5개 확인\n",
    "last_token_logits = logits[0, -1, :]\n",
    "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "top_5_prob, top_5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "print(\"\\n다음 토큰 예측 상위 5개:\")\n",
    "for i, (prob, idx) in enumerate(zip(top_5_prob, top_5_indices)):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"{i+1}. '{token}' (확률: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ub9T5C1Sjj14"
   },
   "source": [
    "- **[주의!] 이상한 토큰들의 정체**\n",
    "    - 출력 결과에서 GPT2는 한글을 바이트 단위로 분해 때문에 토큰의 이상하게 보여질 수 있다⭢의미 손실 발생할 수 있다.\n",
    "- 의미 단위로 분해해야 정확함\n",
    "    - ✅ 원본 텍스트: '프로그래밍은 재미있다. 프로그래밍은'\n",
    "    - 1️⃣ 토큰 분할 결과: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤', '.', 'Ġ', 'í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ']\n",
    "- ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ' ...] 이 토큰들의 정체\n",
    "\n",
    "|정체|설명|문제점|\n",
    "|--- |--- |--- |\n",
    "|UTF-8 바이트의 잘못된 해석 |한글 바이트를 Latin-1로 디코딩한 결과 |의미 완전 손실 |\n",
    "|BPE 알고리즘의 한계 |영어 위주 학습으로 한글 패턴 미학습 |비효율적 토큰화 |\n",
    "|어휘집 부족 |GPT-2 어휘집에 한글 토큰 거의 없음 |알 수 없는 토큰으로 처리 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqqDtMxRkPB3"
   },
   "source": [
    "### **1단계: 문제 상황 정확한 분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1758013706072,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "DjbN4OT2jktt",
    "outputId": "e1c96497-0d34-4508-c880-dc88523859ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트: '프로그래밍은 재미있다'\n",
      "\n",
      "토큰 개수: 28개\n",
      "토큰들: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë']... (처음 10개만 표시)\n",
      "\n",
      "=== 이상한 토큰들의 정체 ===\n",
      "토큰 1: 'í' → ID: 169\n",
      "토큰 2: 'Ķ' → ID: 242\n",
      "토큰 3: 'Ħ' → ID: 226\n",
      "토큰 4: 'ë' → ID: 167\n",
      "토큰 5: '¡' → ID: 94\n",
      "\n",
      "=== UTF-8 바이트 레벨 분석 ===\n",
      "한글 텍스트의 UTF-8 바이트: b'\\xed\\x94\\x84\\xeb\\xa1\\x9c\\xea\\xb7\\xb8\\xeb\\x9e\\x98\\xeb\\xb0\\x8d\\xec\\x9d\\x80 \\xec\\x9e\\xac\\xeb\\xaf\\xb8\\xec\\x9e\\x88\\xeb\\x8b\\xa4'\n",
      "바이트 개수: 31개\n",
      "바이트별 분석:\n",
      "바이트 1: 237 (0xed)\n",
      "바이트 2: 148 (0x94)\n",
      "바이트 3: 132 (0x84)\n",
      "바이트 4: 235 (0xeb)\n",
      "바이트 5: 161 (0xa1)\n",
      "바이트 6: 156 (0x9c)\n",
      "바이트 7: 234 (0xea)\n",
      "바이트 8: 183 (0xb7)\n",
      "바이트 9: 184 (0xb8)\n",
      "바이트 10: 235 (0xeb)\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 한글 텍스트\n",
    "korean_text = \"프로그래밍은 재미있다\"\n",
    "print(f\"원본 텍스트: '{korean_text}'\")\n",
    "print()\n",
    "\n",
    "# 토큰 분할 결과\n",
    "tokens = tokenizer.tokenize(korean_text)\n",
    "print(f\"토큰 개수: {len(tokens)}개\")\n",
    "print(f\"토큰들: {tokens[:10]}... (처음 10개만 표시)\")\n",
    "print()\n",
    "\n",
    "# 이상한 문자들의 정체 확인\n",
    "print(\"=== 이상한 토큰들의 정체 ===\")\n",
    "for i, token in enumerate(tokens[:5]):\n",
    "    # 토큰을 바이트로 변환해보기\n",
    "    try:\n",
    "        token_id = tokenizer.convert_tokens_to_ids([token])[0]\n",
    "        print(f\"토큰 {i+1}: '{token}' → ID: {token_id}\")\n",
    "    except:\n",
    "        print(f\"토큰 {i+1}: '{token}' → 변환 불가\")\n",
    "\n",
    "print()\n",
    "\n",
    "# UTF-8 바이트 분석\n",
    "print(\"=== UTF-8 바이트 레벨 분석 ===\")\n",
    "korean_bytes = korean_text.encode('utf-8')\n",
    "print(f\"한글 텍스트의 UTF-8 바이트: {korean_bytes}\")\n",
    "print(f\"바이트 개수: {len(korean_bytes)}개\")\n",
    "\n",
    "# 각 바이트를 개별 문자로 디코딩 시도\n",
    "print(\"바이트별 분석:\")\n",
    "for i, byte_val in enumerate(korean_bytes[:10]):\n",
    "    print(f\"바이트 {i+1}: {byte_val} (0x{byte_val:02x})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGMcqoTrkRSN"
   },
   "source": [
    "### **2단계: 영어와 한글 토크나이제이션 비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 167,
     "status": "ok",
     "timestamp": 1758013716254,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "jzaUiWdekRcD",
    "outputId": "86b033a6-6e4a-4a10-92d3-db03356a07cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 언어별 토크나이제이션 비교 ===\n",
      "\n",
      "영어: 'Programming is fun'\n",
      "  문자 수: 18\n",
      "  토큰 수: 4\n",
      "  효율성: 0.22 (토큰/문자)\n",
      "  토큰 예시: ['Program', 'ming', 'Ġis', 'Ġfun']...\n",
      "\n",
      "한글: '프로그래밍은 재미있다'\n",
      "  문자 수: 11\n",
      "  토큰 수: 28\n",
      "  효율성: 2.55 (토큰/문자)\n",
      "  토큰 예시: ['í', 'Ķ', 'Ħ', 'ë', '¡']...\n",
      "\n",
      "숫자: '12345'\n",
      "  문자 수: 5\n",
      "  토큰 수: 2\n",
      "  효율성: 0.40 (토큰/문자)\n",
      "  토큰 예시: ['123', '45']...\n",
      "\n",
      "특수문자: 'Hello! @#$%'\n",
      "  문자 수: 11\n",
      "  토큰 수: 5\n",
      "  효율성: 0.45 (토큰/문자)\n",
      "  토큰 예시: ['Hello', '!', 'Ġ@', '#$', '%']...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# 비교 텍스트들\n",
    "texts = {\n",
    "    \"영어\": \"Programming is fun\",\n",
    "    \"한글\": \"프로그래밍은 재미있다\",\n",
    "    \"숫자\": \"12345\",\n",
    "    \"특수문자\": \"Hello! @#$%\"\n",
    "}\n",
    "\n",
    "print(\"=== 언어별 토크나이제이션 비교 ===\")\n",
    "for lang, text in texts.items():\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_count = len(tokens)\n",
    "    char_count = len(text)\n",
    "\n",
    "    print(f\"\\n{lang}: '{text}'\")\n",
    "    print(f\"  문자 수: {char_count}\")\n",
    "    print(f\"  토큰 수: {token_count}\")\n",
    "    print(f\"  효율성: {token_count/char_count:.2f} (토큰/문자)\")\n",
    "    print(f\"  토큰 예시: {tokens[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6wnNFbwkRxS"
   },
   "source": [
    "### **3단계: 올바른 다국어 토크나이저 사용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3991,
     "status": "ok",
     "timestamp": 1758015238394,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "tra_GeXrkR5A",
    "outputId": "e6bf694f-1952-4888-eb3c-144e4cd5f314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 다양한 토크나이저 비교 ===\n",
      "\n",
      "GPT-2 (영어 전용):\n",
      "  토큰 수: 28\n",
      "  토큰들: ['í', 'Ķ', 'Ħ', 'ë', '¡', 'ľ', 'ê', '·', '¸', 'ë', 'ŀ', 'ĺ', 'ë', '°', 'į', 'ìĿ', 'Ģ', 'Ġì', 'ŀ', '¬', 'ë', '¯', '¸', 'ì', 'ŀ', 'Ī', 'ëĭ', '¤']\n",
      "  토큰ID: [169, 242, 226, 167, 94, 250, 166, 115, 116, 167, 252, 246, 167, 108, 235, 35975, 222, 23821, 252, 105, 167, 107, 116, 168, 252, 230, 46695, 97]\n",
      "  역변환: '프로그래밍은 재미있다'\n",
      "  원본과 동일: ✅\n",
      "\n",
      "mBERT (다국어):\n",
      "  토큰 수: 8\n",
      "  토큰들: ['프로', '##그', '##래', '##밍', '##은', '재', '##미', '##있다']\n",
      "  토큰ID: [102574, 78136, 37388, 118960, 10892, 9659, 22458, 76820]\n",
      "  역변환: '프로그래밍은 재미있다'\n",
      "  원본과 동일: ✅\n",
      "\n",
      "XLM-RoBERTa (다국어):\n",
      "  토큰 수: 6\n",
      "  토큰들: ['▁프로', '그래', '밍', '은', '▁재미있', '다']\n",
      "  토큰ID: [54099, 61882, 144667, 697, 157403, 1875]\n",
      "  역변환: '프로그래밍은 재미있다'\n",
      "  원본과 동일: ✅\n",
      "\n",
      "KoBERT (한국어):\n",
      "  토큰 수: 14\n",
      "  토큰들: ['▁', '프로', 'ᄀ', 'ᅳ래ᄆ', 'ᅵ', 'ᆼ', 'ᄋ', 'ᅳᆫ', '▁', '재ᄆ', 'ᅵ', 'ᄋ', 'ᅵ', 'ᆻ다']\n",
      "  토큰ID: [517, 0, 490, 0, 494, 0, 491, 0, 517, 0, 494, 491, 494, 0]\n",
      "  역변환: '프로그래밍은 재미있다'\n",
      "  원본과 동일: ❌\n",
      "\n",
      "KoGPT2 Base v2 (한국어):\n",
      "  토큰 수: 7\n",
      "  토큰들: ['▁프로', '그래', '밍', '은', '▁재미', '있', '다']\n",
      "  토큰ID: [9726, 19561, 7593, 8135, 18767, 8155, 7182]\n",
      "  역변환: '프로그래밍은 재미있다'\n",
      "  원본과 동일: ✅\n",
      "\n",
      "Ko-GPT-Trinity 1.2B (v0.5) (한국어):\n",
      "  토큰 수: 5\n",
      "  토큰들: ['▁프로그래', '밍', '은', '▁재미', '있다']\n",
      "  토큰ID: [49017, 22901, 25768, 32579, 45092]\n",
      "  역변환: '프로그래밍은 재미있다'\n",
      "  원본과 동일: ✅\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 다국어 지원 토크나이저들 비교\n",
    "tokenizer_models = {\n",
    "    \"GPT-2 (영어 전용)\": \"gpt2\",\n",
    "    \"mBERT (다국어)\": \"bert-base-multilingual-cased\",\n",
    "    \"XLM-RoBERTa (다국어)\": \"xlm-roberta-base\",\n",
    "    \"KoBERT (한국어)\" : \"skt/kobert-base-v1\",\n",
    "    \"KoGPT2 Base v2 (한국어)\" : \"skt/kogpt2-base-v2\",   # GPT-2 기반\n",
    "    \"Ko-GPT-Trinity 1.2B (v0.5) (한국어)\" : \"skt/ko-gpt-trinity-1.2B-v0.5\"  # GPT-3 스타일\n",
    "}\n",
    "\n",
    "korean_text = \"프로그래밍은 재미있다\"\n",
    "\n",
    "print(\"=== 다양한 토크나이저 비교 ===\")\n",
    "for model_name, model_id in tokenizer_models.items():\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokens = tokenizer.tokenize(korean_text)\n",
    "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  토큰 수: {len(tokens)}\")\n",
    "        print(f\"  토큰들: {tokens}\")\n",
    "        print(f\"  토큰ID: {token_ids}\")\n",
    "\n",
    "        # 역변환 확인\n",
    "        reconstructed = tokenizer.convert_tokens_to_string(tokens)\n",
    "        print(f\"  역변환: '{reconstructed}'\")\n",
    "        print(f\"  원본과 동일: {'✅' if reconstructed.strip() == korean_text else '❌'}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{model_name}: 로드 실패 - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0_mx4WXldNe"
   },
   "source": [
    "- 한글 처리를 위한 올바른 선택\n",
    "\n",
    "|용도 |권장 모델 |이유\n",
    "|--- |--- |--- |\n",
    "| 한글 텍스트 생성| GPT-3.5/4, KoGPT| 한글 데이터로 훈련됨|\n",
    "| 한글 이해/분류| KoBERT, KoELECTRA| 한국어 특화 모델|\n",
    "| 다국어 처리| mBERT, XLM-RoBERTa| 다국어 동시 지원|\n",
    "| 실습/학습용| 영어 예제 사용| GPT-2 본래 성능 확인|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aj62TQvZl7G5"
   },
   "source": [
    "- 학습용 개선된 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1758013767706,
     "user": {
      "displayName": "Joys",
      "userId": "08210666431620526411"
     },
     "user_tz": -540
    },
    "id": "0TeAXjxwnUQq",
    "outputId": "3bf3f8c9-915b-43e8-e107-2a43b37fdd6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 텍스트: 'Programming is fun. Programming is'\n",
      "토큰들: ['Program', 'ming', 'Ġis', 'Ġfun', '.', 'ĠProgramming', 'Ġis']\n",
      "토큰 수: 7\n",
      "\n",
      "토큰별 의미:\n",
      "1. 'Program' → 의미있는 단위\n",
      "2. 'ming' → 의미있는 단위\n",
      "3. 'Ġis' → 의미있는 단위\n",
      "4. 'Ġfun' → 의미있는 단위\n",
      "5. '.' → 의미있는 단위\n",
      "6. 'ĠProgramming' → 의미있는 단위\n",
      "7. 'Ġis' → 의미있는 단위\n"
     ]
    }
   ],
   "source": [
    "# 올바른 접근: 영어로 실습하기\n",
    "from transformers import GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2가 잘 처리하는 영어 텍스트\n",
    "english_text = \"Programming is fun. Programming is\"\n",
    "print(f\"영어 텍스트: '{english_text}'\")\n",
    "\n",
    "tokens = tokenizer.tokenize(english_text)\n",
    "print(f\"토큰들: {tokens}\")\n",
    "print(f\"토큰 수: {len(tokens)}\")\n",
    "\n",
    "# 각 토큰의 의미 확인\n",
    "print(\"\\n토큰별 의미:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{i+1}. '{token}' → 의미있는 단위\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeP3BFVJf-00"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP6/ucqAZaLnKGXnIxPywUU",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
