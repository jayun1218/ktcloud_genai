{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAPO58qbDYi8"
   },
   "source": [
    "# **transformers 소개**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAmo5uhXQ46c"
   },
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHIcfXn8RArA"
   },
   "source": [
    "- 💡 **NOTE**\n",
    "    - 이 노트북의 코드를 실행하려면 GPU를 사용하는 것이 좋습니다. 구글 코랩에서는 **런타임 > 런타임 유형 변경 > 하드웨어 가속기 > T4 GPU**를 선택하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-OX9xcYMp_f"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCx9g4beNOpa"
   },
   "source": [
    "## **1.transformers란?**\n",
    "\n",
    "\n",
    "- **Hugging Face가 개발한 오픈소스 라이브러리(Transformer 모델 라이브러리)**\n",
    "- 자연어 처리, 컴퓨터 비전, 음성 등 다양한 분야의 최신 Transformer 기반 사전 학습된(pretrained) 모델을 쉽고 빠르게 사용할 수 있도록 지원함\n",
    "- 3개의 딥러닝 라이브러리 지원 :  PyTorch, TensorFlow, Jax\n",
    "- 상세설명 : https://pypi.org/project/transformers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SIij8mNSmaY"
   },
   "source": [
    "\n",
    "- **지원 플랫폼 / 요구사항**\n",
    "    - Python 3.9 이상, PyTorch 2.1+, TensorFlow 2.6+, Flax 0.4.1+\n",
    "- **용도 / 특징**\n",
    "    - 텍스트, 이미지, 오디오, 멀티모달 모델들처럼 다양한 영역의 pretrained 모델을 간단한 API(pipeline)로 빠르게 사용 가능\n",
    "- **추천 사용 케이스**\n",
    "    - 빠르고 쉽게 NLP, 컴퓨터 비전, 음성 등의 사전학습 모델을 사용하거나 fine-tuning 하고자 할 때"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9dDQ0-2SDH2"
   },
   "source": [
    "## **2.설치 방법**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YUQOtVxdYnhM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 4.41.2\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\n"
     ]
    }
   ],
   "source": [
    "# 설치된 transformers 버전 확인  -->(ex: Version: 4.55.4)\n",
    "!pip show transformers | grep Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppzzXeKtBGUX"
   },
   "outputs": [],
   "source": [
    "transformers --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gioKeW2nK2zX"
   },
   "outputs": [],
   "source": [
    "# Phi-3 모델과 호환성 때문에 transformers 4.48.3 버전을 사용합니다.\n",
    "!pip install transformers==4.48.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iBbbJcLvujq4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: 2.8.0\n",
      "Location: /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages\n"
     ]
    }
   ],
   "source": [
    "# 설치된 PyTorch 버전 확인  -->(ex: Version: 2.8.0+cu126 (+cu126-->CUDA 12.6 버전 지원))\n",
    "!pip show torch | grep Version\n",
    "\n",
    "# pip install torch==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rEqIVseOecG"
   },
   "source": [
    "## **3.Transformers 사용 방식(3가지)**\n",
    "- 💡 짧은 코드로 과제에 맞게 사전학습 모델을 다운로드 받고 사용할 수 있음\n",
    "- 💡 간단 기본 코드 중심으로 설명함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1luVOIwjfoY"
   },
   "source": [
    "#### **1.저수준 API (AutoTokenizer + AutoModel)**\n",
    "- 토큰화 → 모델 호출 → **원시 벡터(last_hidden_state, pooler_output) 얻기**\n",
    "- **연구/커스텀 태스크에 적합**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f28jmeSrNE8U"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e1dfb99a334de5933a5820e51957cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7616418c63b045e8b9f313db18aa7e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91167ac2ec044850a7de8464499e31aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752fcbdf2167425bbaf2f0730462b415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aef61df64ed4c78a79a70e752bddbf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 버전\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1.토큰화\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# 2.모델호출\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# 3.원시벡터 얻기\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPqQJA7mskEi"
   },
   "source": [
    "- **AutoTokenizer, TFAutoModel**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF ver: 2.20.0\n",
      "is_tf_available(): False\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import import_utils\n",
    "import tensorflow as tf\n",
    "print(\"TF ver:\", tf.__version__)\n",
    "print(\"is_tf_available():\", import_utils.is_tf_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c_6eAISlNE-i"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFAutoModel requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TFAutoModel\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mgoogle-bert/bert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mTFAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mgoogle-bert/bert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m inputs = tokenizer(\u001b[33m\"\u001b[39m\u001b[33mHello world!\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m outputs = model(**inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KT/ktcloud_genai/whisper-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:2142\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2142\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/KT/ktcloud_genai/whisper-env/lib/python3.12/site-packages/transformers/utils/import_utils.py:2115\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2113\u001b[39m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[32m   2114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n\u001b[32m-> \u001b[39m\u001b[32m2115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH.format(name))\n\u001b[32m   2117\u001b[39m failed = []\n\u001b[32m   2118\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends:\n",
      "\u001b[31mImportError\u001b[39m: \nTFAutoModel requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow 버전\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Hugging Face Transformers 라이브러리에서 PyTorch 모델 가중치를 TensorFlow(TF) 모델로 변환할 때 출력되는 경고문이 출력될 수 있다.\n",
    "#   이 메시지는 PyTorch에서 학습된 BERT 모델의 일부 가중치(weight)가 TFBertModel 초기화 과정에서 사용되지 않았다는 뜻으로\n",
    "#   본문의 경우 TFBertModel에 필요한 모든 가중치는 잘 로드되었고,\n",
    "#   사용하지 않은 가중치는 단지 다른 task용 헤드일 뿐이므로,\n",
    "#   추가 학습 없이 바로 예측에 사용 가능하다는 얘기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEKOGQ-G5LMQ"
   },
   "source": [
    "- **토크나이저 옵션/속성 확인 방법**\n",
    "모델마다 다양한 옵션/속성을 갖고 있다.\n",
    "    - **print(tokenizer)** → 주요 설정(모델 이름, vocab 크기, do_lower_case, max_len 등) 출력\n",
    "    - **tokenizer.init_kwargs** → 초기화 시점의 옵션이 dict 형태로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jNQ94je5KF1"
   },
   "outputs": [],
   "source": [
    "# 토크나이저의 모든 속성과 설정 보기\n",
    "print(tokenizer)\n",
    "print('-'* 50)\n",
    "\n",
    "# 딕셔너리 형태로 옵션 확인\n",
    "print(tokenizer.init_kwargs)\n",
    "print('-'* 50)\n",
    "\n",
    "print('# 단어 집합 크기: ', tokenizer.vocab_size)               # 단어 집합 크기\n",
    "print('# 입력 토큰 최대 길이: ', tokenizer.model_max_length)    # 입력 토큰 최대 길이\n",
    "print('# 소문자 변환 여부: ', tokenizer.do_lower_case)          # 소문자 변환 여부\n",
    "print('# [PAD] 토큰: ', tokenizer.pad_token)                    # [PAD] 토큰\n",
    "print('# [CLS] 토큰: ', tokenizer.cls_token)                    # [CLS] 토큰\n",
    "print('# [SEP] 토큰: ', tokenizer.sep_token)                    # [SEP] 토큰\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lSZxcbXjf04"
   },
   "source": [
    "#### **2.중간 수준 API (Task-specific Models)**\n",
    "- **AutoModelForXXX → 태스크별 학습/추론용**\n",
    "- AutoModelForSequenceClassification, AutoModelForQuestionAnswering 등 태스크 맞춤 헤드가 붙은 모델 사용\n",
    "- 장점: 파인튜닝/태스크 전용 학습에 적합\n",
    "- 단점: 태스크별 클래스 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpZCfe1_kzMj"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNUnXmVIjf-f"
   },
   "source": [
    "#### **3.고수준 API (pipeline)**\n",
    "- **pipeline → 빠른 프로토타이핑, 데모용**\n",
    "- 토크나이저+모델+후처리를 한 번에 묶어 제공\n",
    "- 감정분석, 번역, 요약, QA 등 바로 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4ZkU_Hklm1A"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "print(nlp(\"I love this course!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNBhIvnzPGYD"
   },
   "source": [
    "## **4.예제로 QuickTour**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-yv_vjYTEn9"
   },
   "source": [
    "### **예제: 감정분석**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kbODkmM0PL28"
   },
   "outputs": [],
   "source": [
    "# 감정 분석 파이프라인\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "print(classifier('We are very happy to introduce pipeline to the transformers repository.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Dqi289eTARx"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "print(nlp(\"이 강의는 정말 재미있어요!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ADhlYgBTKT8"
   },
   "source": [
    "### **예제: 이미지 객체 감지**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8GYQrUE5pBmX"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import pipeline\n",
    "\n",
    "# 이미지 다운로드\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n",
    "image_data = requests.get(url, stream=True).raw\n",
    "image = Image.open(image_data)\n",
    "\n",
    "# 객체 감지 파이프라인\n",
    "object_detector = pipeline(\"object-detection\")\n",
    "results = object_detector(image)\n",
    "\n",
    "# 원본 이미지에 바운딩 박스 그리기\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for result in results:\n",
    "    box = result[\"box\"]\n",
    "    label = result[\"label\"]\n",
    "    score = result[\"score\"]\n",
    "\n",
    "    # 박스 좌표\n",
    "    x1, y1, x2, y2 = box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]\n",
    "\n",
    "    # 박스 그리기\n",
    "    draw.rectangle(((x1, y1), (x2, y2)), outline=\"red\", width=3)\n",
    "    # 레이블 + 점수 표시\n",
    "    draw.text((x1, y1 - 10), f\"{label} {score:.2f}\", fill=\"yellow\")\n",
    "\n",
    "# 결과 출력 (Colab 환경에서 시각화)\n",
    "image.show()   # 로컬 환경이면 새 창\n",
    "display(image) # Colab/Jupyter 환경이면 노트북 안에 표시\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O89lFdMSmASC"
   },
   "source": [
    "##  **5.huggingface Access Token 적용 유무**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJDOLfJQ8vkU"
   },
   "source": [
    "#### **Access Token 없이 공개 모델 다운로드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWEKGelDlAWF"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6peF7JXdmLD"
   },
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "# PyTorch 체크포인트에서 불러오기\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\", from_pt=True)\n",
    "\n",
    "# 정상 동작 (경고는 나올 수 있음)\n",
    "outputs = model(**tokenizer(\"Hello world\", return_tensors=\"tf\"))\n",
    "print(outputs.last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPeGS1yncJm0"
   },
   "source": [
    " #### **Access Token 넣고 비공개 모델 접근**\n",
    "- https://huggingface.co/settings/tokens  로그인 후 사용하고자 하는 모델에 대한 access용 토큰 발급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVqNfPWtvC2E"
   },
   "outputs": [],
   "source": [
    "# 1단계: Hugging Face 토큰 생성 및 설정\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Google Colab에서 토큰 입력 (한 번만 실행)\n",
    "# https://huggingface.co/settings/tokens 에서 토큰 생성 후 입력\n",
    "login()\n",
    "\n",
    "# 또는 직접 토큰 입력 (보안상 권장하지 않음)\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8RA42UEcJy1"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login(\"hf_xxxxxxxxxxxxx\")\n",
    "\n",
    "# from transformers import AutoModel\n",
    "# model = AutoModel.from_pretrained(\"username/private-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVvKuqY1tKG9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZK4AjHIij89"
   },
   "source": [
    "## **6.BERT 모델로 기본 코드 상세 설명**\n",
    "- 💡 Transformers 라이브러리 기본 사용법 - BERT 모델 예제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PKY4z-qVqRt7"
   },
   "source": [
    "\n",
    "1. 토크나이저의 주요 기능:\n",
    "    - 텍스트를 토큰으로 분할 (서브워드 토큰화)\n",
    "    - 토큰을 숫자 ID로 변환\n",
    "    - 특수 토큰 추가 ([CLS], [SEP], [PAD] 등)\n",
    "    - 어텐션 마스크 생성\n",
    "2. 모델의 주요 구성요소:\n",
    "    - Embedding Layer: 토큰 ID를 벡터로 변환\n",
    "    - Transformer Layers: 12개의 인코더 층\n",
    "    - Pooler: [CLS] 토큰의 표현을 가공 (분류 작업용)\n",
    "3. 입력 텍스트 토큰화 단계별 과정:\n",
    "    - 텍스트 정규화: \"Hello world!\" → \"hello world!\"\n",
    "    - 서브워드 토큰화: [\"hello\", \"world\", \"!\"]\n",
    "    - 특수 토큰 추가: [\"[CLS]\", \"hello\", \"world\", \"!\", \"[SEP]\"]\n",
    "    - ID 변환: [101, 7592, 2088, 999, 102] (예시)\n",
    "    - 텐서 생성 및 패딩/마스킹\n",
    "4. 모델 추론(BERT 모델 내부 처리 과정):\n",
    "    - Embedding: 토큰 ID → 768차원 벡터\n",
    "    - Position Encoding: 위치 정보 추가\n",
    "    - 12개 Transformer Layer 통과:\n",
    "        - Multi-Head Self-Attention\n",
    "        - Feed-Forward Network\n",
    "        - Layer Normalization\n",
    "        - Residual Connection\n",
    "    - 최종 hidden state 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOqZ4l6rr3oQ"
   },
   "source": [
    "#### 기본 코드 상세 : 저수준 API (AutoTokenizer + AutoModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M73QeBZXjMRE"
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Transformers 라이브러리 기본 사용법 - BERT 모델 예제\n",
    "# =====================================================\n",
    "\n",
    "# 필요한 라이브러리 import\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# AutoTokenizer: 자동으로 적절한 토크나이저를 로드하는 클래스\n",
    "# AutoModel: 자동으로 적절한 모델 아키텍처를 로드하는 클래스\n",
    "\n",
    "# =====================================================\n",
    "# 1. 토크나이저 초기화\n",
    "# =====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\"\"\"\n",
    "토크나이저 초기화 상세 설명:\n",
    "- from_pretrained(): Hugging Face Hub에서 사전 훈련된 토크나이저를 다운로드\n",
    "- \"google-bert/bert-base-uncased\": 모델 식별자\n",
    "  * google-bert: Google에서 개발한 BERT 모델\n",
    "  * bert-base: 기본 크기 모델 (12층, 768 hidden size)\n",
    "  * uncased: 대소문자를 구분하지 않음 (모든 텍스트를 소문자로 변환)\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# 2. 모델 초기화\n",
    "# =====================================================\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\"\"\"\n",
    "모델 초기화 상세 설명:\n",
    "- 같은 모델 식별자를 사용하여 토크나이저와 모델의 일치성 보장\n",
    "- BERT-base 모델 구조:\n",
    "  * 12개의 Transformer 인코더 층\n",
    "  * 768차원 히든 스테이트\n",
    "  * 12개의 어텐션 헤드\n",
    "  * 30,522개의 어휘 크기\n",
    "  * 약 110M개의 파라미터\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# 3. 입력 텍스트 토큰화\n",
    "# =====================================================\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "\"\"\"\n",
    "토큰화 과정 상세 설명:\n",
    "- 입력: \"Hello world!\" (일반 텍스트 문자열)\n",
    "- return_tensors=\"pt\": PyTorch 텐서 형태로 반환 지정\n",
    "\n",
    "텍스트 정규화: \"Hello world!\" → \"hello world!\"\n",
    "- 서브워드 토큰화: [\"hello\", \"world\", \"!\"]\n",
    "- 특수 토큰 추가: [\"[CLS]\", \"hello\", \"world\", \"!\", \"[SEP]\"]\n",
    "- ID 변환: [101, 7592, 2088, 999, 102] (예시)\n",
    "- 텐서 생성 및 패딩/마스킹\n",
    "\n",
    "반환되는 딕셔너리 구조:\n",
    "- input_ids: 토큰 ID 시퀀스 [1, 5]\n",
    "- token_type_ids: 문장 구분자 (단일 문장이므로 모두 0) [1, 5]\n",
    "- attention_mask: 실제 토큰 위치 표시 (모두 1) [1, 5]\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# 4. 모델 추론 실행\n",
    "# =====================================================\n",
    "outputs = model(**inputs)\n",
    "\"\"\"\n",
    "모델 추론 상세 설명:\n",
    "- **inputs: 딕셔너리를 키워드 인자로 전달\n",
    "- model()은 forward() 메서드를 호출\n",
    "\n",
    "BERT 모델 내부 처리 과정:\n",
    "1. Embedding: 토큰 ID → 768차원 벡터\n",
    "2. Position Encoding: 위치 정보 추가\n",
    "3. 12개 Transformer Layer 통과:\n",
    "   - Multi-Head Self-Attention\n",
    "   - Feed-Forward Network\n",
    "   - Layer Normalization\n",
    "   - Residual Connection\n",
    "4. 최종 hidden state 생성\n",
    "\n",
    "반환되는 객체 (BaseModelOutputWithPoolingAndCrossAttentions):\n",
    "- last_hidden_state: 마지막 층의 모든 토큰 표현 [batch_size, seq_len, hidden_size]\n",
    "- pooler_output: [CLS] 토큰의 풀링된 표현 [batch_size, hidden_size]\n",
    "- hidden_states: 모든 층의 hidden state (output_hidden_states=True일 때)\n",
    "- attentions: 어텐션 가중치 (output_attentions=True일 때)\n",
    "\"\"\"\n",
    "\n",
    "# =====================================================\n",
    "# 5. 출력 결과 확인\n",
    "# =====================================================\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\"\"\"\n",
    "출력 결과 분석:\n",
    "- 예상 결과: torch.Size([1, 5, 768])\n",
    "  * 1: 배치 크기 (단일 문장)\n",
    "  * 5: 시퀀스 길이 (토큰 개수)\n",
    "    - [CLS] + hello + world + ! + [SEP] = 5개\n",
    "  * 768: BERT-base의 히든 차원\n",
    "\n",
    "각 차원의 의미:\n",
    "- 첫 번째 차원 (1): 배치에서 처리된 문장 수\n",
    "- 두 번째 차원 (5): 각 토큰의 위치\n",
    "- 세 번째 차원 (768): 각 토큰의 768차원 벡터 표현\n",
    "\n",
    "토큰별 표현 접근:\n",
    "- outputs.last_hidden_state[0, 0, :]: [CLS] 토큰 표현\n",
    "- outputs.last_hidden_state[0, 1, :]: \"hello\" 토큰 표현\n",
    "- outputs.last_hidden_state[0, 2, :]: \"world\" 토큰 표현\n",
    "- outputs.last_hidden_state[0, 3, :]: \"!\" 토큰 표현\n",
    "- outputs.last_hidden_state[0, 4, :]: [SEP] 토큰 표현\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9Se8Uegrxso"
   },
   "source": [
    "#### 기본 코드 활용 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kh9hYgSjVws"
   },
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 추가 활용 예제\n",
    "# =====================================================\n",
    "from typing import List, Tuple\n",
    "import torch        # tensor와 그래디언트 제어를 위해 사용\n",
    "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
    "\n",
    "# ===== 환경/리소스 설정 =====\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\").to(DEVICE).eval()\n",
    "\n",
    "\n",
    "# ===== 1) 토큰화 분석 =====\n",
    "def analyze_tokenization(text: str, tokenizer: AutoTokenizer) -> None:\n",
    "    \"\"\"\n",
    "    단일 텍스트를 한 번의 인코딩으로 분석(특수 토큰/ids/토큰/복원 텍스트).\n",
    "    \"\"\"\n",
    "    enc: BatchEncoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=None,\n",
    "        return_attention_mask=False,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "\n",
    "    input_ids = enc[\"input_ids\"]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    decoded = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "\n",
    "    print(f\"원본 텍스트 : {text}\")\n",
    "    print(f\"토큰들      : {tokens}\")\n",
    "    print(f\"토큰 ID들   : {input_ids}\")\n",
    "    print(f\"디코딩 결과 : {decoded}\")\n",
    "\n",
    "\n",
    "# ===== 2) 표현 추출(Mean/Max/CLS) =====\n",
    "def extract_representations(\n",
    "    outputs: BaseException,  # 실제 타입은 ModelOutput이지만, 런타임 제네릭 회피\n",
    "    attention_mask: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    모델 출력에서 (CLS, mean-pool, max-pool) 반환.\n",
    "    - CLS: 각 배치의 첫 토큰 임베딩\n",
    "    - Mean: attention_mask를 반영해 PAD 제외 평균\n",
    "    - Max: 시퀀스 차원 최대값\n",
    "    반환 shape: (B, H), (B, H), (B, H)\n",
    "    \"\"\"\n",
    "    last_hidden = outputs.last_hidden_state  # (B, L, H)\n",
    "    # B (Batch size) : 한 번에 처리하는 문장(또는 입력 샘플)의 개수\n",
    "    # L (Sequence length) : 각 입력 문장의 토큰 개수 (패딩 포함)\n",
    "    # H (Hidden size / Hidden dimension) : 각 토큰을 벡터로 표현한 차원 수, bert-base-uncased의 경우 H = 768, bert-large는 H = 1024\n",
    "\n",
    "    # CLS\n",
    "    cls_repr = last_hidden[:, 0, :]  # (B, H)\n",
    "\n",
    "    # Mean pooling (PAD 제외). mask -> (B, L, 1), 가중 합/합계\n",
    "    mask = attention_mask.unsqueeze(-1).type_as(last_hidden)  # (B, L, 1)\n",
    "    summed = (last_hidden * mask).sum(dim=1)                   # (B, H)\n",
    "    counts = mask.sum(dim=1).clamp(min=1e-9)                  # (B, 1) zero-division 방지\n",
    "    mean_repr = summed / counts                               # (B, H)\n",
    "\n",
    "    # Max pooling (PAD는 -inf로 채워서 무시)\n",
    "    masked_hidden = last_hidden.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    max_repr = masked_hidden.max(dim=1).values                # (B, H)\n",
    "\n",
    "    return cls_repr, mean_repr, max_repr\n",
    "\n",
    "# ===== 3) 배치 처리 =====\n",
    "def batch_processing(\n",
    "    texts: List[str],\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModel,\n",
    "    max_length: int = 512\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    여러 텍스트를 배치로 처리하고 (CLS/Mean/Max) 임베딩을 반환.\n",
    "    \"\"\"\n",
    "    enc: BatchEncoding = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # 디바이스 이동\n",
    "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "    print(enc)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**enc)\n",
    "\n",
    "    cls_repr, mean_repr, max_repr = extract_representations(outputs, enc[\"attention_mask\"])\n",
    "    return cls_repr, mean_repr, max_repr\n",
    "\n",
    "\n",
    "# ===== 사용 예시 =====\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"[INFO] Using device: {DEVICE}\")\n",
    "\n",
    "    # 1) 토큰 정보 자세히 보기\n",
    "    analyze_tokenization(\"Hello world!\", tokenizer)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 2) 배치 처리 + 표현 추출\n",
    "    sample_texts = [\n",
    "        \"Hello world!\",\n",
    "        \"How are you today?\",\n",
    "        \"This is a longer sentence for demonstration.\"\n",
    "    ]\n",
    "    cls_vec, mean_vec, max_vec = batch_processing(sample_texts, tokenizer, model)\n",
    "\n",
    "    print(f\"CLS 표현     : {cls_vec.shape}\")   # (B, 768)\n",
    "    print(f\"Mean 풀링    : {mean_vec.shape}\")  # (B, 768)\n",
    "    print(f\"Max  풀링    : {max_vec.shape}\")   # (B, 768)\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jffyGqil6qrn"
   },
   "source": [
    "### **특수 토큰 설명** : BERT [CLS], [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZX8c8BslW9B"
   },
   "source": [
    "\n",
    "\n",
    "####  **BERT 소개**\n",
    "\n",
    "- **BERT**(Bidirectional Encoder Representations from Transformers)\n",
    "    - 구글(2018)**에서 발표한 사전학습(pre-trained) 언어모델\n",
    "    - 양방향(Bidirectional) Transformer 인코더 구조를 사용 → 문맥을 앞뒤 모두 반영\n",
    "    - 사전학습(Pre-training) 두 가지 방식\n",
    "        - Masked Language Modeling (MLM): 문장 속 단어를 [MASK]로 가리고 예측\n",
    "        - Next Sentence Prediction (NSP): 두 문장이 연속되는지 여부 예측\n",
    "    - 학습 후 다양한 NLP 태스크(분류, 개체명 인식, 질의응답 등)에 파인튜닝(fine-tuning) 가능\n",
    "    - BERT는 구조적으로 두 개의 문장(A, B)까지만 명시적으로 입력(구분) 가능함 -->(세그먼트 ID는 0과 1만 지원 가능)\n",
    "        - 문장이 3개 이상일 경우: 보통 하나의 긴 시퀀스로 합치고 [SEP] 토큰으로 분리함\n",
    "    - 성능 혁신-->다른 파생 모델 등장\n",
    "        - RoBERTa: NSP 제거, 문장 구분을 다르게 처리\n",
    "        - Longformer, BigBird: 긴 문장/문서 처리\n",
    "        - ALBERT: Sentence Order Prediction (SOP) 사용\n",
    "- [참고] **GPT** : Transformer의 Decoder 블록만 사용하는 구조\n",
    "    - 단방향 (왼쪽 → 오른쪽만 봄), 오른쪽(과거) 문맥만 보고 다음 단어를 예측하는 자기회귀(auto-regressive) 언어모델\n",
    "    - 즉, GPT는 문장을 입력받으면:\n",
    "        1. 문장을 토큰화 (예: \"Hello world\" → [Hello, world])\n",
    "        2. 각 토큰을 임베딩 + 위치 임베딩\n",
    "        3. Causal Masked Self-Attention 사용\n",
    "            - 현재 단어를 예측할 때 앞쪽 단어만 참조 가능\n",
    "            - 예: \"The cat sat on the\" → 다음 단어 mat 예측\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiREd0zVz0TA"
   },
   "source": [
    "####  **[CLS] 토큰**\n",
    "- **[CLS] 토큰의 역할과 특징**\n",
    "    1. 위치: 항상 입력 시퀀스의 맨 앞에 위치\n",
    "    2. 목적: 전체 문장의 집약된 표현(aggregate representation) 생성\n",
    "    3. 활용: 문장 수준의 분류 작업 (감정 분석, 문서 분류 등)에 주로 사용\n",
    "    4. 학습: NSP(Next Sentence Prediction) 과제를 통해 문장 간 관계 학습\n",
    "    - **[CLS] 토큰의 임베딩은 문장 전체의 의미를 압축한 768차원 벡터**입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK8pOdi2ypXH"
   },
   "source": [
    "#### **[SEP] 토큰**\n",
    "\n",
    "- **[SEP] 토큰의 역할과 특징**\n",
    "    1. 위치: 각 문장의 끝에 위치\n",
    "    2. 목적: 문장 간 경계를 명확히 구분\n",
    "    3. 활용:\n",
    "        - 단일 문장: 문장 끝을 표시\n",
    "        - 문장 쌍: 두 문장을 구분 (질의응답, 문장 관계 분석)\n",
    "    4. 토큰 타입 ID: [SEP] 토큰을 기준으로 문장 구분 (0, 1로 표시)\n",
    "    - **[SEP] 토큰은 BERT가 문장의 구조를 이해하는 데 핵심적인 역할**을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bm1dMhANG6nJ"
   },
   "source": [
    "#### **[CLS], [SEP] 토큰 확인**\n",
    "- [참고] Claude 아티팩트 : https://claude.ai/public/artifacts/8b6bd242-86a5-48e9-b7dd-457dba23781d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeG6xCUc2Os_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "\n",
    "# 감정 분석용 예제 문장들\n",
    "sentences = [\n",
    "    \"I love this product! It's amazing!\",           # 매우 긍정\n",
    "    \"This is okay, nothing special.\",               # 중립\n",
    "    \"I hate this. Worst purchase ever.\",           # 매우 부정\n",
    "    \"The service was good but could be better.\",   # 약간 긍정\n",
    "    \"Today is a beautiful day! 😊\",                # 이모지 포함 긍정\n",
    "    \"I'm not sure how I feel about this...\",       # 애매한 감정\n",
    "]\n",
    "\n",
    "# =====================================================\n",
    "# 1. [CLS] 토큰 (Classification Token)\n",
    "# =====================================================\n",
    "def demonstrate_cls_token():\n",
    "\n",
    "    cls_embeddings = []\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # 토큰화 및 모델 추론\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # [CLS] 토큰의 임베딩 추출 (인덱스 0)\n",
    "        cls_embedding = outputs.last_hidden_state[0, 0, :]  # [배치, 위치, 차원]\n",
    "        cls_embeddings.append(cls_embedding)\n",
    "\n",
    "        print()\n",
    "        print(f\"문장 {i+1}: {sentence}\")\n",
    "        print(f\"[CLS] 임베딩 형태: {cls_embedding.shape}\")\n",
    "        print(f\"[CLS] 임베딩 일부: {cls_embedding[:5].tolist()}\")  # 처음 5개 값만 출력\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # [CLS] 토큰 간 유사도 계산 (코사인 유사도)\n",
    "    print(\"\\n=== [CLS] 토큰 간 유사도 분석 ===\")\n",
    "    for i in range(len(cls_embeddings)):\n",
    "        for j in range(i+1, len(cls_embeddings)):\n",
    "            similarity = F.cosine_similarity(\n",
    "                cls_embeddings[i].unsqueeze(0),\n",
    "                cls_embeddings[j].unsqueeze(0)\n",
    "            )\n",
    "            print(f\"문장 {i+1} vs 문장 {j+1}: {similarity.item():.4f}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. [SEP] 토큰 (Separator Token)\n",
    "# =====================================================\n",
    "def demonstrate_sep_token():\n",
    "\n",
    "    # 1.단일 문장 예제\n",
    "    single_sentence = \"BERT is a powerful language model.\"\n",
    "    inputs_single = tokenizer(single_sentence, return_tensors=\"pt\")\n",
    "\n",
    "    print(\"1. 단일 문장에서의 [SEP] 토큰:\")\n",
    "    print(f\"원본 문장: {single_sentence}\")\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs_single.input_ids[0])\n",
    "    print(f\"토큰들: {tokens}\")\n",
    "    print(f\"토큰 타입 ID: {inputs_single.token_type_ids[0].tolist()}\")\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # 2.문장 쌍 예제 (질의응답 형태)\n",
    "    question = \"What is BERT?\"\n",
    "    answer = \"BERT is a bidirectional encoder representation from transformers.\"\n",
    "\n",
    "    # 문장 쌍 토큰화\n",
    "    inputs_pair = tokenizer(question, answer, return_tensors=\"pt\")\n",
    "\n",
    "    print(\"2. 문장 쌍에서의 [SEP] 토큰:\")\n",
    "    print(f\"질문: {question}\")\n",
    "    print(f\"답변: {answer}\")\n",
    "    tokens_pair = tokenizer.convert_ids_to_tokens(inputs_pair.input_ids[0])\n",
    "    print(f\"토큰들: {tokens_pair}\")\n",
    "    print(f\"토큰 타입 ID: {inputs_pair.token_type_ids[0].tolist()}\")\n",
    "    print(\"-\" * 50)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # [SEP] 토큰 위치 찾기\n",
    "    sep_token_id = tokenizer.sep_token_id\n",
    "    sep_positions = (inputs_pair.input_ids[0] == sep_token_id).nonzero().flatten()\n",
    "    print(f\"[SEP] 토큰 ID: {sep_token_id}\")\n",
    "    print(f\"[SEP] 토큰 위치: {sep_positions.tolist()}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 3. 특수 토큰 활용 패턴 비교\n",
    "# =====================================================\n",
    "def compare_token_patterns():\n",
    "    \"\"\"다양한 입력 패턴에서 특수 토큰 사용 비교\"\"\"\n",
    "\n",
    "    print(\"\\n=== 특수 토큰 사용 패턴 비교 ===\")\n",
    "\n",
    "    examples = [\n",
    "        # 케이스 1: 단일 짧은 문장\n",
    "        (\"Hello world!\", \"단일 짧은 문장\"),\n",
    "\n",
    "        # 케이스 2: 단일 긴 문장\n",
    "        (\"Natural language processing is a fascinating field that combines linguistics and computer science.\", \"단일 긴 문장\"),\n",
    "\n",
    "        # 케이스 3: 문장 쌍\n",
    "        ((\"Is this a good movie?\", \"Yes, it's excellent!\"), \"문장 쌍\"),\n",
    "    ]\n",
    "\n",
    "    for example, description in examples:\n",
    "        print(f\"\\n--- {description} ---\")\n",
    "\n",
    "        if isinstance(example, tuple):  # 문장 쌍\n",
    "            inputs = tokenizer(example[0], example[1], return_tensors=\"pt\")\n",
    "            print(f\"문장 1: {example[0]}\")\n",
    "            print(f\"문장 2: {example[1]}\")\n",
    "        else:  # 단일 문장\n",
    "            inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "            print(f\"문장: {example}\")\n",
    "\n",
    "        # 토큰 분석\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "        token_types = inputs.token_type_ids[0].tolist()\n",
    "\n",
    "        print(f\"토큰 개수: {len(tokens)}\")\n",
    "        print(f\"토큰들: {tokens}\")\n",
    "        print(f\"토큰 타입: {token_types}\")\n",
    "\n",
    "        # 특수 토큰 위치 확인\n",
    "        cls_pos = tokens.index('[CLS]') if '[CLS]' in tokens else -1\n",
    "        sep_positions = [i for i, token in enumerate(tokens) if token == '[SEP]']\n",
    "\n",
    "        print(f\"[CLS] 위치: {cls_pos}\")\n",
    "        print(f\"[SEP] 위치: {sep_positions}\")\n",
    "\n",
    "def print_token_id():\n",
    "    # 실제 토큰 ID 값 확인\n",
    "    print(f\"\\n=== 토큰 ID 값 === \")\n",
    "    print(f\"   [CLS] 토큰 ID: {tokenizer.cls_token_id}\")\n",
    "    print(f\"   [SEP] 토큰 ID: {tokenizer.sep_token_id}\")\n",
    "    print(f\"   [PAD] 토큰 ID: {tokenizer.pad_token_id}\")\n",
    "    print(f\"   [UNK] 토큰 ID: {tokenizer.unk_token_id}\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 실행\n",
    "# =====================================================\n",
    "\n",
    "demonstrate_cls_token()     # [CLS] 토큰의 역할과 특징\n",
    "demonstrate_sep_token()     # [SEP] 토큰의 역할과 특징\n",
    "compare_token_patterns()    # 특수 토큰 활용 패턴 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmO_uvi663pq"
   },
   "source": [
    "- **=== 특수 토큰 사용 주의사항 ===**\n",
    "    1. [CLS] 토큰 주의사항:\n",
    "        - 항상 시퀀스의 첫 번째 위치(인덱스 0)에 위치\n",
    "        - 문장 수준 작업에만 사용 (토큰 수준 작업에는 부적합)\n",
    "        - 파인튜닝시 [CLS] 토큰에 분류 헤드 연결\n",
    "    2. [SEP] 토큰 주의사항:\n",
    "        - 문장 쌍 처리시 반드시 필요\n",
    "        - 토큰 타입 ID와 함께 문장 구분에 사용\n",
    "        - 최대 시퀀스 길이 계산시 [SEP] 토큰도 포함하여 계산\n",
    "    3. 일반적인 실수들:\n",
    "        - [CLS] 토큰을 토큰 수준 분류에 사용\n",
    "        - 문장 쌍에서 [SEP] 토큰 누락\n",
    "        - 토큰 타입 ID 무시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eau2XiCJqlJr"
   },
   "source": [
    "## **7.사전 학습된 모델 사용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgpMcFQs7W4Z"
   },
   "source": [
    "**=== 사전 훈련 모델 vs 랜덤 분류기 비교 ===**\n",
    "- ✅ 사전 훈련된 모델의 장점:\n",
    "    - • 실제 감정 분석 데이터셋으로 훈련되어 높은 정확도\n",
    "    - • 다양한 표현과 문맥을 이해\n",
    "    - • 이모지, 속어, 은유적 표현도 처리 가능\n",
    "    - • 신뢰할 수 있는 확률 분포 제공\n",
    "\n",
    "- ❌ 랜덤 가중치 분류기의 한계:\n",
    "    - • 무작위 예측으로 정확도 약 33% (3클래스 기준)\n",
    "    - • 문맥을 전혀 이해하지 못함\n",
    "    - • 일관성 없는 결과\n",
    "\n",
    "\n",
    "- **📚 핵심 학습 포인트**:\n",
    "   1. [CLS] 토큰은 문장 전체의 표현을 담고 있습니다\n",
    "   2. 실제 감정 분석은 사전 훈련된 모델을 사용해야 정확합니다\n",
    "   3. 랜덤 가중치로는 의미있는 분류가 불가능합니다\n",
    "\n",
    "- [참고] Claude 아티팩트 : https://claude.ai/public/artifacts/f019b7d3-01de-467e-a01d-00ca8bf4fdb4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qewQ-4IrK9V5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "\n",
    "# 감정 분석용 예제 문장들\n",
    "sentences = [\n",
    "    \"I love this product! It's amazing!\",           # 매우 긍정\n",
    "    \"This is okay, nothing special.\",               # 중립\n",
    "    \"I hate this. Worst purchase ever.\",           # 매우 부정\n",
    "    \"The service was good but could be better.\",   # 약간 긍정\n",
    "]\n",
    "\n",
    "# =====================================================\n",
    "# 방법 1: 랜덤 가중치 분류기 (교육용)\n",
    "# =====================================================\n",
    "def sentiment_analysis_with_cls():\n",
    "    \"\"\"[CLS] 토큰을 활용한 간단한 감정 분석 예제\"\"\"\n",
    "\n",
    "    print(\"\\n=== 랜덤 가중치 분류기(토큰 활용 감정 분석) ===\")\n",
    "\n",
    "    cls_embeddings = []\n",
    "\n",
    "    # 간단한 감정 분류를 위한 선형 분류기 (3개 클래스: 부정, 중립, 긍정)\n",
    "    # 실제로는 훈련된 가중치를 사용해야 하지만, 여기서는 시연용으로 랜덤 가중치 사용\n",
    "    torch.manual_seed(42)  # 재현 가능한 결과를 위한 시드 설정\n",
    "    classifier_head = torch.nn.Linear(768, 3)  # 768차원 입력 → 3개 클래스 출력 # ← 이 부분이 랜덤 가중치 생성!(내부적으로 가중치 자동 초기화)\n",
    "\n",
    "    # 실제 감정 라벨 (이 모델의 경우)\n",
    "    sentiment_labels = [\"부정적 (NEGATIVE)\", \"중립적 (NEUTRAL)\", \"긍정적 (POSITIVE)\"]\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # [CLS] 토큰 임베딩 추출\n",
    "            cls_embedding = outputs.last_hidden_state[0, 0, :]\n",
    "            cls_embeddings.append(cls_embedding)\n",
    "\n",
    "            # 감정 분류 예측 수행\n",
    "            logits = classifier_head(cls_embedding.unsqueeze(0))  # 배치 차원 추가\n",
    "            probabilities = F.softmax(logits, dim=-1)  # 확률로 변환\n",
    "            predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "            confidence = probabilities[0, predicted_class].item()\n",
    "\n",
    "            print(f\"문장 {i+1}: {sentence}\")\n",
    "            print(f\"[CLS] 임베딩 크기: {cls_embedding.shape}\")\n",
    "            print(f\"예측된 감정: {sentiment_labels[predicted_class]}\")\n",
    "            print(f\"신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
    "\n",
    "            # 각 감정별 확률 분포 출력\n",
    "            print(\"감정별 확률 분포:\")\n",
    "            for j, (label, prob) in enumerate(zip(sentiment_labels, probabilities[0])):\n",
    "                    bar_length = int(prob.item() * 20)  # 시각적 표현을 위한 바\n",
    "                    bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "                    print(f\"  {label}: {prob.item():.4f} ({prob.item()*100:.1f}%) {bar}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# # 방법 2: 사전 훈련된 감정 분석 모델 (실용적)\n",
    "# =====================================================\n",
    "def sentiment_analysis_with_cls_pretrainedmodel():\n",
    "    \"\"\"사전 훈련된 감정 분석 모델을 활용한 실제 감정 분석 예제\"\"\"\n",
    "\n",
    "    print(\"\\n=== 사전 훈련된 감정 분석 모델 활용 ===\")\n",
    "\n",
    "    try:\n",
    "        # 실제 감정 분석용 사전 훈련 모델 로드\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "        model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "        print(f\"모델 로딩 중: {model_name}\")\n",
    "        print(\"(처음 실행시 모델 다운로드로 시간이 걸릴 수 있습니다...)\")\n",
    "\n",
    "        sentiment_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name) # ← 이 부분이 사전 훈련된 가중치\n",
    "\n",
    "        print(\"모델 로딩 완료! 감정 분석을 시작합니다.\\n\")\n",
    "\n",
    "        # 실제 감정 라벨 (이 모델의 경우)\n",
    "        sentiment_labels = [\"부정적 (NEGATIVE)\", \"중립적 (NEUTRAL)\", \"긍정적 (POSITIVE)\"]\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            # 사전 훈련된 모델의 토크나이저 사용 (중요!)\n",
    "            inputs = sentiment_tokenizer(sentence, return_tensors=\"pt\",\n",
    "                                       truncation=True, padding=True, max_length=512)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # 🔧 수정된 부분: output_hidden_states=True 명시적으로 설정\n",
    "                outputs = sentiment_model(**inputs, output_hidden_states=True)\n",
    "\n",
    "                # 예측 결과 처리\n",
    "                logits = outputs.logits\n",
    "                probabilities = F.softmax(logits, dim=-1)\n",
    "                predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "                confidence = probabilities[0, predicted_class].item()\n",
    "\n",
    "                # 🔧 수정된 부분: 안전한 [CLS] 토큰 임베딩 추출\n",
    "                cls_embedding = None\n",
    "                if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:\n",
    "                    try:\n",
    "                        # RoBERTa는 <s> 토큰을 사용 (BERT의 [CLS]와 동일한 역할)\n",
    "                        cls_embedding = outputs.hidden_states[-1][0, 0, :]  # 마지막 레이어, 첫 번째 토큰\n",
    "                        cls_shape = cls_embedding.shape\n",
    "                    except (IndexError, TypeError) as e:\n",
    "                        print(f\"   ⚠️ [CLS] 토큰 추출 실패: {e}\")\n",
    "                        cls_embedding = None\n",
    "                else:\n",
    "                    print(\"   ⚠️ hidden_states를 사용할 수 없습니다.\")\n",
    "\n",
    "                print(f\"문장 {i+1}: {sentence}\")\n",
    "                if cls_embedding is not None:\n",
    "                    print(f\"[CLS] 임베딩 크기: {cls_shape}\")\n",
    "                    print(f\"[CLS] 임베딩 샘플: {cls_embedding[:5].detach().numpy()}\")\n",
    "                print(f\"예측된 감정: {sentiment_labels[predicted_class]}\")\n",
    "                print(f\"신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
    "\n",
    "                # 각 감정별 확률 분포 출력\n",
    "                print(\"감정별 확률 분포:\")\n",
    "                for j, (label, prob) in enumerate(zip(sentiment_labels, probabilities[0])):\n",
    "                    bar_length = int(prob.item() * 20)  # 시각적 표현을 위한 바\n",
    "                    bar = \"█\" * bar_length + \"░\" * (20 - bar_length)\n",
    "                    print(f\"  {label}: {prob.item():.4f} ({prob.item()*100:.1f}%) {bar}\")\n",
    "\n",
    "                print(\"-\" * 80)\n",
    "\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"⚠️  필요한 라이브러리를 찾을 수 없습니다: {e}\")\n",
    "        print(\"다음 명령으로 transformers 라이브러리를 설치해주세요:\")\n",
    "        print(\"pip install transformers torch\")\n",
    "        print(\"대안으로 키워드 기반 감정 분석을 수행합니다...\\n\")\n",
    "        sentiment_analysis_with_keywords(sentences)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  모델 로딩 중 오류 발생: {e}\")\n",
    "        print(\"🔧 문제 해결 방법:\")\n",
    "        print(\"   1. 인터넷 연결 확인 (모델 다운로드 필요)\")\n",
    "        print(\"   2. transformers 라이브러리 버전 확인: pip install --upgrade transformers\")\n",
    "        print(\"   3. torch 라이브러리 설치: pip install torch\")\n",
    "        print(\"   4. 디스크 공간 확인 (모델 파일이 약 500MB)\")\n",
    "        print(\"\\n대안으로 키워드 기반 감정 분석을 수행합니다...\\n\")\n",
    "        sentiment_analysis_with_keywords(sentences)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 실행\n",
    "# =====================================================\n",
    "\n",
    "# 감정 분석 비교 (두 가지 방법)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"감정 분석 방법 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 방법 1: 랜덤 가중치 분류기 (교육용)\n",
    "sentiment_analysis_with_cls()\n",
    "\n",
    "\n",
    "# 방법 2: 사전 훈련된 감정 분석 모델 (실용적)\n",
    "sentiment_analysis_with_cls_pretrainedmodel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efyIiSlWJBQ1"
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis_with_keywords():\n",
    "    \"\"\"키워드 기반 간단한 감정 분석 (폴백 방법)\"\"\"\n",
    "\n",
    "    print(\"=== 키워드 기반 간단 감정 분석 ===\")\n",
    "\n",
    "    # 간단한 감정 키워드 사전\n",
    "    positive_words = {\n",
    "        'love', 'amazing', 'great', 'good', 'excellent', 'wonderful',\n",
    "        'fantastic', 'awesome', 'perfect', 'best', 'beautiful', 'happy'\n",
    "    }\n",
    "\n",
    "    negative_words = {\n",
    "        'hate', 'terrible', 'awful', 'bad', 'worst', 'horrible',\n",
    "        'disgusting', 'sad', 'angry', 'disappointed', 'annoying'\n",
    "    }\n",
    "\n",
    "    sentences = [\n",
    "        \"I love this product! It's amazing!\",           # 매우 긍정\n",
    "        \"This is okay, nothing special.\",               # 중립\n",
    "        \"I hate this. Worst purchase ever.\",           # 매우 부정\n",
    "        \"The service was good but could be better.\",   # 약간 긍정\n",
    "    ]\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = sentence.lower().replace('!', '').replace('.', '').replace(',', '').split()\n",
    "\n",
    "        positive_count = sum(1 for word in words if word in positive_words)\n",
    "        negative_count = sum(1 for word in words if word in negative_words)\n",
    "\n",
    "        if positive_count > negative_count:\n",
    "            sentiment = \"긍정적\"\n",
    "            confidence = min(0.9, 0.6 + (positive_count - negative_count) * 0.1)\n",
    "        elif negative_count > positive_count:\n",
    "            sentiment = \"부정적\"\n",
    "            confidence = min(0.9, 0.6 + (negative_count - positive_count) * 0.1)\n",
    "        else:\n",
    "            sentiment = \"중립적\"\n",
    "            confidence = 0.5\n",
    "\n",
    "        print(f\"문장 {i+1}: {sentence}\")\n",
    "        print(f\"긍정 키워드: {positive_count}개, 부정 키워드: {negative_count}개\")\n",
    "        print(f\"예측된 감정: {sentiment}\")\n",
    "        print(f\"신뢰도: {confidence:.4f} ({confidence*100:.1f}%)\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# =====================================================\n",
    "# 실행\n",
    "# =====================================================\n",
    "# 키워드 기반 간단 감정 분석\n",
    "sentiment_analysis_with_keywords(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF_03YnsiwPi"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olZXCZdcWh_x"
   },
   "source": [
    "## **Phi‑3**\n",
    "\n",
    "- **Microsoft가 개발한 소형 언어 모델(SLM) 시리즈**\n",
    "    - Phi‑3‑mini (3.8B 파라미터), Phi‑3‑small (약 7B), Phi‑3‑medium (14B) 등 다양한 변형을 포함\n",
    "- 파라미터 & 컨텍스트 윈도우\n",
    "    - Phi-3-mini: 3.8B 파라미터, 4K 및 확장된 128K 컨텍스트 윈도우 지원\n",
    "    - Phi‑3‑small: 약 7B, 기본 8K 컨텍스트\n",
    "    - Phi‑3‑medium: 14B 기능, 더 넓은 컨텍스트 처리 능력\n",
    "- 학습 데이터 & 미세조정\n",
    "    - 총 3.3조 토큰 이상으로 구성된 고품질 필터링 웹 데이터, 합성 데이터 등을 활용. 이후 **Supervised Fine-Tuning (SFT)**과 Direct Preference Optimization (DPO) 기법을 통해 인간 선호도 및 안전 기준에 맞춰 미세 조정됨\n",
    "- 성능 지표\n",
    "    - 최소형 모델(Phi-3-mini)도 MMLU 69%, MT-bench 8.38 등 성능으로 Mixtral 8x7B, GPT-3.5와 유사한 수준\n",
    "    - Phi‑3‑small 및 medium는 각각 MMLU 75% 및 78%, MT‑bench 8.7 및 8.9를 달성\n",
    "    - 여러 벤치마크(HellaSwag, WinoGrande, TruthfulQA, HumanEval 등)에서 강력한 성능을 보임\n",
    "- 이용 가능 플랫폼 및 최적화\n",
    "    - Azure AI Studio, Hugging Face, Ollama에서 사용 가능\n",
    "    - ONNX Runtime과 Windows DirectML을 통한 GPU/CPU 및 모바일 기기 최적화 지원\n",
    "    - Ollama를 통해 로컬에서도 실행 가능하며, 긴 문맥 처리가 특징\n",
    "- 확장 및 응용성\n",
    "    - 리소스 제약 환경, 저지연 요구 시나리오, 강력한 추론/수학/로직 처리, 긴 컨텍스트 필요 작업에 적합\n",
    "    - Strathweb Phi Engine처럼 다양한 플랫폼(C#, Swift, Kotlin 등)에서 로컬 실행을 쉽게 해 주는 라이브러리도 존재\n",
    "- 책임있는 AI 설계\n",
    "    - Microsoft의 Responsible AI 기준에 따라 설계됨. RLHF, 자동화 테스트, red-teaming 등을 포함한 안전성 검증 프로세스가 적용됨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYYPM8LKYKtD"
   },
   "source": [
    "### QuickTour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiJCX0VgX5F3"
   },
   "source": [
    "#### 모델 로드 및 토큰 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ye9p2FAuak2-"
   },
   "source": [
    "- 중간수준 API(Task-specific Models) 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZfCMj2BYK4z"
   },
   "outputs": [],
   "source": [
    "# Phi-3-mini 로드 및 간단 생성 (Transformers)\n",
    "#   선형적 생성 흐름을 체험하며 모델 작동 원리를 설명하기 좋다.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c9r0tymaW76"
   },
   "source": [
    "- 고수준 API(pipeline) 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Foxafz3aXEY"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 파이프라인을 만듭니다.\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxUo-N0cbC_H"
   },
   "source": [
    "#### 프롬프트를 작성하고 모델에 주입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJOl9GZIbDJp"
   },
   "outputs": [],
   "source": [
    "# 프롬프트 (사용자 입력 / 쿼리)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}\n",
    "]\n",
    "\n",
    "# 출력 생성\n",
    "output = generator(messages)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-x5CFg5mYDhy"
   },
   "source": [
    "#### 수학적 추론 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c32AwHKIYmPK"
   },
   "outputs": [],
   "source": [
    "# 수학적 추론 테스트\n",
    "#   직접 알고리즘 사고가 나오는 과정을 볼 수 있어 교육적으로 유익(?)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "\n",
    "# 프롬프트 (사용자 입력 / 쿼리)\n",
    "prompt = \"시곗바늘이 15분 동안 얼마나 움직일까요?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IezMGMJIY6Lb"
   },
   "source": [
    "### [실습] Phi-3 4K vs 128K 컨텍스트 비교 실습\n",
    "**[주의!]** 모델 크기가 크기 때문에 128K 모델은 실행 속도가 느릴 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAmWuompZN_4"
   },
   "source": [
    "**실험 포인트**\n",
    "\n",
    "1. 4K 버전 (Phi-3-mini-4k)\n",
    "    - 4,096 토큰까지만 인식\n",
    "    - 긴 문맥(30K 토큰)을 넣으면 앞부분은 잘라버림 → “정보 손실” 발생\n",
    "    - 따라서 답변이 틀리거나 불완전할 수 있음\n",
    "2. 128K 버전 (Phi-3-mini-128k)\n",
    "    - 최대 128,000 토큰까지 인식\n",
    "    - 긴 문맥 전체를 유지\n",
    "    - 답변에서 정확한 문장 개수나 더 깊은 문맥 이해가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgA_FG5IZFYO"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 4K 버전과 128K 버전 각각 불러오기\n",
    "models = {\n",
    "    \"4k\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"128k\": \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "loaded_models = {}\n",
    "\n",
    "for key, model_name in models.items():\n",
    "    tokenizers[key] = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    loaded_models[key] = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, trust_remote_code=True, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "# 긴 텍스트 생성 (테스트용 문맥)\n",
    "long_text = \"이 문장은 테스트 문장입니다. \" * 3000  # 약 30K 토큰 수준\n",
    "\n",
    "prompt = long_text + \"\\n\\n질문: 위 문장의 개수는 몇 개인가요?\"\n",
    "\n",
    "# 두 모델에 각각 입력\n",
    "for key in [\"4k\", \"128k\"]:\n",
    "    tokenizer = tokenizers[key]\n",
    "    model = loaded_models[key]\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    print(f\"\\n===== Phi-3 {key.upper()} 결과 =====\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAnHmaGmZoPd"
   },
   "source": [
    "<!-- ### [미션] Phi 비교하기\n",
    "1. 문학 작품(예: 헌법 전문 + 긴 소설 일부)을 통째로 넣어 보고, 요약을 비교하기\n",
    "2. 회의록 20페이지 vs 100페이지를 넣어 두 버전에서 요약 결과 차이를 직접 확인\n",
    "3. 숫자 세기, 문맥 요약 같은 단순 과제를 통해 차이를 눈으로 보여주기 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufq_HCjKZ-C5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0g36MizoMox8"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "whisper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
